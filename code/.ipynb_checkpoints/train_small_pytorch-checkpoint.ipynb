{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net for Semantic Image Segmentation\n",
    "#### Kaggle Competition (https://www.kaggle.com/c/cvpr-2018-autonomous-driving)\n",
    "#### Stanford course CS231n\n",
    "#### Aristos Athens (aristos@stanford.edu)\n",
    "#### Alice Li (ali2@stanford.edu)\n",
    "#### Amelia Bian (qb1@stanford.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import gc\n",
    "import psutil\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import keras\n",
    "# from keras import backend as K\n",
    "# from tensorflow.python.keras.applications import vgg16, inception_v3, resnet50, mobilenet\n",
    "# from tensorflow.python.keras.applications.vgg16 import VGG16\n",
    "# from tensorflow.python.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "%matplotlib inline\n",
    "CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "    \"\"\"\n",
    "    Object to hold all hyperparameters. Makes passing parameters between functions easier.\n",
    "    Many of these might not be strictly necessary.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # General params\n",
    "        self.data_type = tf.float32\n",
    "        self.root = '../data/cvpr-2018-autonomous-driving/train_color'\n",
    "        self.device = '/cpu:0'\n",
    "        \n",
    "        # Training params\n",
    "        self.optimizer = \"Adam\"    # options: SGD, RMSProp, Momentum, Adam, Adagrad\n",
    "        self.learning_rate = 5e-3\n",
    "        self.lr_decay = 0.99\n",
    "        self.loss_type = \"full\"    # options: \"fast\", \"full\"\n",
    "        self.momentum = 0.9\n",
    "        self.use_Nesterov = True\n",
    "        self.init_scale = 4.0\n",
    "        \n",
    "        self.print_images = True   # if True, disaply grayscale image of best prediction every train_batch. !Consumes memory\n",
    "        self.verbose = True        # if True, print info about accuracy calculations\n",
    "        \n",
    "        # Data loader params\n",
    "        self.shuffle_data = True   # Currently doesn't do anything\n",
    "        self.num_classes = 20      # This value is probably wrong\n",
    "        self.preload = False\n",
    "        \n",
    "        # Total data to train on = num_epochs*batch_size\n",
    "        self.batch_size = 2\n",
    "        self.num_epochs = 30    \n",
    "        self.num_files_to_load = self.num_epochs * self.batch_size\n",
    "        \n",
    "        # Saved model params\n",
    "        self.save_model = True\n",
    "        self.use_saved_model = False\n",
    "        \n",
    "              \n",
    "hp = HyperParameters()\n",
    "USE_GPU = False\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we create a customized data loader for the CVPR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVPR(object):\n",
    "    \"\"\"\n",
    "    A customized data loader for CVPR.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform=None, batch_size = 10):\n",
    "        \"\"\" Intialize the CVPR dataset\n",
    "        \n",
    "        Args:\n",
    "            - root: root directory of the dataset\n",
    "            - tranform: a custom tranform function\n",
    "            - preload: if preload the dataset into memory\n",
    "        \"\"\"\n",
    "        # Important - This tracks which batch we are on. Used for loading as we go (as opposed to preloading)\n",
    "        self.batch_num = 0\n",
    "        \n",
    "        # read filenames\n",
    "        self.filenames = []\n",
    "        filenames = glob.glob(osp.join(hp.root, '*.jpg'))\n",
    "        np.random.shuffle(filenames)\n",
    "        for fn in filenames:\n",
    "            lbl = fn[:-4] + '_instanceIds.png'\n",
    "            lbl = lbl.replace('color', 'label')\n",
    "            self.filenames.append((fn, lbl)) # (filename, label) pair\n",
    "            if len(self.filenames) >= hp.num_files_to_load: \n",
    "                break\n",
    "                \n",
    "        self.labels = []\n",
    "        self.images = []\n",
    "        if hp.preload:\n",
    "            self._preload()\n",
    "            \n",
    "        self.len = len(self.filenames)\n",
    "                              \n",
    "    def _preload(self):\n",
    "        \"\"\"\n",
    "        Preload dataset to memory\n",
    "        \"\"\"\n",
    "        for image_fn, label in self.filenames:\n",
    "            # load images\n",
    "            image = Image.open(image_fn)\n",
    "            self.images.append(np.asarray(image))            # avoid too many opened files bug\n",
    "            image.close()\n",
    "            # load labels\n",
    "            image = Image.open(label)\n",
    "            self.labels.append((np.asarray(image)/1000).astype(int))             # avoid too many opened files bug\n",
    "            image.close()\n",
    "    \n",
    "        assert len(self.images) == len(self.labels), 'Got different numbers of data and labels'\n",
    "#         assert self.images[0].shape == self.labels[0].shape, 'Got data and labels with different shapes'\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset\n",
    "        \"\"\"\n",
    "        if self.images is not None:\n",
    "            # If dataset is preloaded\n",
    "            image = self.images[index]\n",
    "            label = self.labels[index]\n",
    "        else:\n",
    "            # If on-demand data loading\n",
    "            image_fn, label = self.filenames[index]\n",
    "            image = Image.open(image_fn)\n",
    "            label = Image.open(label)\n",
    "            \n",
    "        # May use transform function to transform samples\n",
    "        # e.g., random crop, whitening\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "        # return image and label\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "#     def __iter__(self):\n",
    "#         N, B = hp.num_files_to_load, hp.batch_size\n",
    "#         return iter((self.images[i:i+B], self.labels[i:i+B]) for i in range(0, N, B))\n",
    "    \n",
    "    def update_iterator(self):\n",
    "        # If a batch has already been loaded free the last batch\n",
    "        if len(self.images) != 0:\n",
    "            for image in self.images:\n",
    "                del image\n",
    "            for label in self.labels:\n",
    "                del label\n",
    "        del self.images\n",
    "        del self.labels\n",
    "        self.images = []\n",
    "        self.labels =[]\n",
    "        gc.collect()\n",
    "        i = self.batch_num*hp.batch_size\n",
    "        train_filenames = self.filenames[i:i + hp.batch_size]\n",
    "        for image_fn, label in train_filenames:\n",
    "            image = Image.open(image_fn)\n",
    "            self.images.append(self.normalize_image(np.asarray(image)))\n",
    "            image.close()\n",
    "            del image\n",
    "            image = Image.open(label)\n",
    "            self.labels.append((np.asarray(image)/1000).astype(int))\n",
    "            image.close()\n",
    "            del image\n",
    "        gc.collect()\n",
    "        self.batch_num += 1\n",
    "        \n",
    "    def get_images(self):\n",
    "        return self.images, self.labels\n",
    "\n",
    "    def normalize_image(self, image):\n",
    "        # Normalize the data: subtract the mean pixel and divide by std\n",
    "        mean_pixel = image.mean(keepdims=True)\n",
    "        std_pixel = image.std(keepdims=True)\n",
    "        image = (image - mean_pixel) / std_pixel\n",
    "        return image\n",
    "    \n",
    "    def reset_data_set(self):\n",
    "        self.batch_num = 0\n",
    "        np.random.shuffle(self.filenames)\n",
    "        del self.images\n",
    "        del self.labels\n",
    "        gc.collect()\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(inputs, is_training):\n",
    "    \n",
    "    initializer = tf.variance_scaling_initializer(scale=hp.init_scale)\n",
    "#     initializer = tf.contrib.layers.xavier_initializer()\n",
    "    input_shape = (2710, 3384, 3)\n",
    "    \n",
    "    # Transfer learning example here:\n",
    "    # https://deeplearningsandbox.com/how-to-use-transfer-learning-and-fine-tuning-in-keras-and-tensorflow-to-build-an-image-recognition-94b0b02444f2\n",
    "#     base = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    \n",
    "    # Not 100% sure what this line does\n",
    "#     model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     from pympler import asizeof\n",
    "#     print(asizeof.asizeof(base))\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "#     for layer in base.layers:\n",
    "#         layer.trainable = False\n",
    "#     model.add(base)\n",
    "    \n",
    "    # Add our own layers to the layers list within the model\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(0.01)\n",
    "    layers = [\n",
    "        # Downsampling layers\n",
    "        tf.keras.layers.Conv2D(5, (3, 3), kernel_initializer = initializer, input_shape=input_shape, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(4, 4), strides = (4, 4), padding = \"valid\"),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(12, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.Conv2D(16, (5, 5), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides = (2, 2), padding = \"valid\"),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(24, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        \n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides = (2, 2), padding = \"valid\"),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "\n",
    "        # Upsampling layers\n",
    "        tf.keras.layers.UpSampling2D(size=(2, 2)),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(32, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        \n",
    "        tf.keras.layers.UpSampling2D(size=(2, 2)),\n",
    "        tf.keras.layers.ZeroPadding2D(padding=((0,1),(1,1))),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(24, (5, 5), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.Conv2D(16, (5, 5), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        \n",
    "        tf.keras.layers.UpSampling2D(size=(4, 4)),\n",
    "        tf.keras.layers.ZeroPadding2D(padding=((1,1),(0,0))),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(12, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.Conv2D(10, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.Conv2D(1, (3, 3), activation=None, kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.0)\n",
    "        \n",
    "    ]\n",
    "#     model.add(layers)\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "\n",
    "    return model(inputs)\n",
    "\n",
    "def create_optimizer(hp):\n",
    "    \n",
    "    optimizer = None\n",
    "    if hp.optimizer == \"Momentum\":    \n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate=hp.learning_rate, momentum=hp.momentum, use_nesterov=hp.use_Nesterov)\n",
    "    if hp.optimizer == \"Adam\":    \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=hp.learning_rate, beta1=hp.lr_decay)\n",
    "    if hp.optimizer == \"AdaGrad\":\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate=hp.learning_rate)\n",
    "    if hp.optimizer == \"SGD\":\n",
    "        optimizer = tf.train.SGDOptimizer(learning_rate=hp.learning_rate, lr_decay=hp.lr_decay)\n",
    "    if hp.optimizer == \"RMSProp\":\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=hp.learning_rate, decay=hp.lr_decay, momentum=hp.momentum, epsilon=1e-10)\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "\n",
      "Starting training loop...\n",
      "\n",
      "Starting batch 0...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-26bc067f16ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0mdata_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCVPR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-26bc067f16ae>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(hp, data_set, create_model, create_optimizer)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Batch %d  |  Loss = %.4f  |  Positive Accuracy = %.4f  |  Total Accuracy = %.4f\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_net(hp, data_set, create_model, create_optimizer):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
    "    accuracy on the CIFAR-10 validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - create_model: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = create_model()\n",
    "    - create_optimizer: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = create_optimizer()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints progress during training\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "        \n",
    "    with tf.device(hp.device):\n",
    "        # Construct the computational graph we will use to train the model. We\n",
    "        # use the create_model to construct the model, declare placeholders for\n",
    "        # the data and labels\n",
    "        x = tf.placeholder(hp.data_type, [None, 2710, 3384, 3])\n",
    "#         y = tf.placeholder(np.dtype('int32'), [None, 2710, 3384])\n",
    "        y = tf.placeholder(hp.data_type, [None, 2710, 3384])\n",
    "\n",
    "        # We need a place holder to explicitly specify if the model is in the training\n",
    "        # phase or not. This is because a number of layers behaves differently in\n",
    "        # training and in testing, e.g., dropout and batch normalization.\n",
    "        # We pass this variable to the computation graph through feed_dict as shown below.\n",
    "        is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "        # Use the model function to build the forward pass.\n",
    "        output = tf.squeeze(create_model(x, is_training))\n",
    "\n",
    "        # Full loss type is better\n",
    "        if hp.loss_type == \"full\":\n",
    "#             loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=y, dim=0))\n",
    "            loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=output, targets=y, pos_weight=40))\n",
    "#             loss = 1/tf.reduce_mean(tf.cast(tf.equal(tf.cast(tf.squeeze(output), tf.int32), tf.squeeze(y)), tf.int32))\n",
    "        if hp.loss_type == \"fast\":\n",
    "            loss = tf.reduce_mean(tf.square(y - output))\n",
    "#             loss = tf.reduce_mean(y - output)\n",
    "\n",
    "        # Use the create_optimizer to construct an Optimizer, then use the optimizer\n",
    "        # to set up the training step. Asking TensorFlow to evaluate the\n",
    "        # train_op returned by optimizer.minimize(loss) will cause us to make a\n",
    "        # single update step using the current minibatch of data.\n",
    "\n",
    "        # Note that we use tf.control_dependencies to force the model to run\n",
    "        # the tf.GraphKeys.UPDATE_OPS at each training step. tf.GraphKeys.UPDATE_OPS\n",
    "        # holds the operators that update the states of the network.\n",
    "        # For example, the tf.layers.batch_normalization function adds the running mean\n",
    "        # and variance update operators to tf.GraphKeys.UPDATE_OPS.\n",
    "\n",
    "        optimizer = create_optimizer(hp)\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer_step = optimizer.minimize(loss)\n",
    "\n",
    "                \n",
    "                \n",
    "    # Now we can run the computational graph many times to train the model.\n",
    "    # When we call sess.run we ask it to evaluate train_op, which causes the model to update.\n",
    "    # # Total data to train on = num_epochs*batch_size\n",
    "    \n",
    "#     data_iterator = data_set.__iter__()\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    train_output = None    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        if hp.use_saved_model:\n",
    "            load_saved_model(sess)   \n",
    "        if hp.save_model:\n",
    "             builder = create_saver(sess) \n",
    "                  \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"\\nStarting training loop...\\n\")\n",
    "        \n",
    "        for epoch in range(hp.num_epochs):\n",
    "            print('Starting batch %d...\\n' % epoch)\n",
    "            data_set.update_iterator()\n",
    "            train_images, train_labels = data_set.get_images()\n",
    "            feed_dict = {x: train_images, y: train_labels, is_training: 1}\n",
    "            loss_value, _, train_output = sess.run([loss, optimizer_step, output], feed_dict=feed_dict)\n",
    "            accuracy, total_acc = check_accuracy(hp, sess, train_output, train_labels)\n",
    "            print('Batch %d  |  Loss = %.4f  |  Positive Accuracy = %.4f  |  Total Accuracy = %.4f\\n' % (epoch, loss_value, accuracy, total_acc))\n",
    "            gc.collect()\n",
    "#             print_graph_size()\n",
    "                \n",
    "        data_set.reset_data_set()\n",
    "        save_model(sess, builder)\n",
    "    \n",
    "\n",
    "data_set = CVPR(hp.root, transform = tf.convert_to_tensor)              \n",
    "train_net(hp, data_set, create_model, create_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
