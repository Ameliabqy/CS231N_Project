{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net for Semantic Image Segmentation\n",
    "#### Kaggle Competition (https://www.kaggle.com/c/cvpr-2018-autonomous-driving)\n",
    "#### Stanford course CS231n\n",
    "#### Aristos Athens (aristos@stanford.edu)\n",
    "#### Alice Li (ali2@stanford.edu)\n",
    "#### Amelia Bian (qb1@stanford.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import gc\n",
    "import psutil\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.python.keras.applications import vgg16, inception_v3, resnet50, mobilenet\n",
    "from tensorflow.python.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.python.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "%matplotlib inline\n",
    "CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "    \"\"\"\n",
    "    Object to hold all hyperparameters. Makes passing parameters between functions easier.\n",
    "    Many of these might not be strictly necessary.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # General params\n",
    "        self.data_type = tf.float32\n",
    "        self.root = '../data/cvpr-2018-autonomous-driving/train_color'\n",
    "        self.device = '/cpu:0'\n",
    "        \n",
    "        # Training params\n",
    "        self.optimizer = \"Adam\"    # options: SGD, RMSProp, Momentum, Adam, Adagrad\n",
    "        self.learning_rate = 5e-4\n",
    "        self.lr_decay = 0.99\n",
    "        self.loss_type = \"full\"    # options: \"fast\", \"full\"\n",
    "        self.momentum = 0.9\n",
    "        self.use_Nesterov = True\n",
    "        self.init_scale = 6.0\n",
    "        self.num_epochs = 30       # Total data to train on = num_epochs*batch_size\n",
    "        \n",
    "        self.print_images = False\n",
    "        self.verbose = True\n",
    "        \n",
    "        # Data loader params\n",
    "        self.shuffle_data = True   # Currently doesn't do anything\n",
    "        self.num_classes = 20      # This value is probably wrong\n",
    "        self.preload = False\n",
    "        self.batch_size = 12\n",
    "        self.num_files_to_load = self.num_epochs * self.batch_size\n",
    "        \n",
    "        # Saved model params\n",
    "        self.save_model = True\n",
    "        self.use_saved_model = False\n",
    "        \n",
    "              \n",
    "hp = HyperParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we create a customized data loader for the CVPR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVPR(object):\n",
    "    \"\"\"\n",
    "    A customized data loader for CVPR.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform=None, batch_size = 10):\n",
    "        \"\"\" Intialize the CVPR dataset\n",
    "        \n",
    "        Args:\n",
    "            - root: root directory of the dataset\n",
    "            - tranform: a custom tranform function\n",
    "            - preload: if preload the dataset into memory\n",
    "        \"\"\"\n",
    "        # Important - This tracks which batch we are on. Used for loading as we go (as opposed to preloading)\n",
    "        self.batch_num = 0\n",
    "        \n",
    "        # read filenames\n",
    "        self.filenames = []\n",
    "        filenames = glob.glob(osp.join(hp.root, '*.jpg'))\n",
    "        np.random.shuffle(filenames)\n",
    "        for fn in filenames:\n",
    "            lbl = fn[:-4] + '_instanceIds.png'\n",
    "            lbl = lbl.replace('color', 'label')\n",
    "            self.filenames.append((fn, lbl)) # (filename, label) pair\n",
    "            if len(self.filenames) >= hp.num_files_to_load: \n",
    "                break\n",
    "                \n",
    "        self.labels = []\n",
    "        self.images = []\n",
    "        if hp.preload:\n",
    "            self._preload()\n",
    "            \n",
    "        self.len = len(self.filenames)\n",
    "                              \n",
    "    def _preload(self):\n",
    "        \"\"\"\n",
    "        Preload dataset to memory\n",
    "        \"\"\"\n",
    "        for image_fn, label in self.filenames:\n",
    "            # load images\n",
    "            image = Image.open(image_fn)\n",
    "            self.images.append(np.asarray(image))            # avoid too many opened files bug\n",
    "            image.close()\n",
    "            # load labels\n",
    "            image = Image.open(label)\n",
    "            self.labels.append((np.asarray(image)/1000).astype(int))             # avoid too many opened files bug\n",
    "            image.close()\n",
    "    \n",
    "        assert len(self.images) == len(self.labels), 'Got different numbers of data and labels'\n",
    "#         assert self.images[0].shape == self.labels[0].shape, 'Got data and labels with different shapes'\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset\n",
    "        \"\"\"\n",
    "        if self.images is not None:\n",
    "            # If dataset is preloaded\n",
    "            image = self.images[index]\n",
    "            label = self.labels[index]\n",
    "        else:\n",
    "            # If on-demand data loading\n",
    "            image_fn, label = self.filenames[index]\n",
    "            image = Image.open(image_fn)\n",
    "            label = Image.open(label)\n",
    "            \n",
    "        # May use transform function to transform samples\n",
    "        # e.g., random crop, whitening\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "        # return image and label\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        N, B = hp.num_files_to_load, hp.batch_size\n",
    "        return iter((self.images[i:i+B], self.labels[i:i+B]) for i in range(0, N, B))\n",
    "    \n",
    "    def update_iterator(self):\n",
    "        # If a batch has already been loaded free the last batch\n",
    "        if len(self.images) != 0:\n",
    "            for image in self.images:\n",
    "                del image\n",
    "            for label in self.labels:\n",
    "                del label\n",
    "        del self.images\n",
    "        del self.labels\n",
    "        self.images = []\n",
    "        self.labels =[]\n",
    "        gc.collect()\n",
    "        i = self.batch_num*hp.batch_size\n",
    "        train_filenames = self.filenames[i:i + hp.batch_size]\n",
    "        for image_fn, label in train_filenames:\n",
    "            image = Image.open(image_fn)\n",
    "            self.images.append(self.normalize_image(np.asarray(image)))\n",
    "            image.close()\n",
    "            del image\n",
    "            image = Image.open(label)\n",
    "            self.labels.append((np.asarray(image)/1000).astype(int))\n",
    "            image.close()\n",
    "            del image\n",
    "        gc.collect()\n",
    "        self.batch_num += 1\n",
    "        \n",
    "    def get_images(self):\n",
    "        return self.images, self.labels\n",
    "\n",
    "    def normalize_image(self, image):\n",
    "        # Normalize the data: subtract the mean pixel and divide by std\n",
    "        mean_pixel = image.mean(keepdims=True)\n",
    "        std_pixel = image.std(keepdims=True)\n",
    "        image = (image - mean_pixel) / std_pixel\n",
    "        return image\n",
    "    \n",
    "    def reset_data_set(self):\n",
    "        self.batch_num = 0\n",
    "        np.random.shuffle(self.bfilenames)\n",
    "        del self.images\n",
    "        del self.labels\n",
    "        gc.collect()\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(inputs, is_training):\n",
    "    \n",
    "    initializer = tf.variance_scaling_initializer(scale=hp.init_scale)\n",
    "#     initializer = tf.contrib.layers.xavier_initializer()\n",
    "    input_shape = (2710, 3384, 3)\n",
    "    \n",
    "    # Transfer learning example here:\n",
    "    # https://deeplearningsandbox.com/how-to-use-transfer-learning-and-fine-tuning-in-keras-and-tensorflow-to-build-an-image-recognition-94b0b02444f2\n",
    "#     base = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    \n",
    "    # Not 100% sure what this line does\n",
    "#     model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     from pympler import asizeof\n",
    "#     print(asizeof.asizeof(base))\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "#     for layer in base.layers:\n",
    "#         layer.trainable = False\n",
    "#     model.add(base)\n",
    "    \n",
    "    # Add our own layers to the layers list within the model\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(0.01)\n",
    "    layers = [\n",
    "        # Downsampling layers\n",
    "        tf.keras.layers.Conv2D(5, (3, 3), kernel_initializer = initializer, input_shape=input_shape, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(4, 4), strides = (4, 4), padding = \"valid\"),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(12, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.Conv2D(16, (5, 5), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides = (2, 2), padding = \"valid\"),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(24, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        \n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides = (2, 2), padding = \"valid\"),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "\n",
    "        # Upsampling layers\n",
    "        tf.keras.layers.UpSampling2D(size=(2, 2)),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(32, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        \n",
    "        tf.keras.layers.UpSampling2D(size=(2, 2)),\n",
    "        tf.keras.layers.ZeroPadding2D(padding=((0,1),(1,1))),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(24, (5, 5), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.Conv2D(16, (5, 5), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        \n",
    "        tf.keras.layers.UpSampling2D(size=(4, 4)),\n",
    "        tf.keras.layers.ZeroPadding2D(padding=((1,1),(0,0))),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(12, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.Conv2D(10, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.Conv2D(1, (3, 3), activation=None, kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.0)\n",
    "        \n",
    "    ]\n",
    "#     model.add(layers)\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "\n",
    "    return model(inputs)\n",
    "\n",
    "def create_optimizer(hp):\n",
    "    \n",
    "    optimizer = None\n",
    "    if hp.optimizer == \"Momentum\":    \n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate=hp.learning_rate, momentum=hp.momentum, use_nesterov=hp.use_Nesterov)\n",
    "    if hp.optimizer == \"Adam\":    \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=hp.learning_rate, beta1=hp.lr_decay)\n",
    "    if hp.optimizer == \"AdaGrad\":\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate=hp.learning_rate)\n",
    "    if hp.optimizer == \"SGD\":\n",
    "        optimizer = tf.train.SGDOptimizer(learning_rate=hp.learning_rate, lr_decay=hp.lr_decay)\n",
    "    if hp.optimizer == \"RMSProp\":\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=hp.learning_rate, decay=hp.lr_decay, momentum=hp.momentum, epsilon=1e-10)\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "\n",
      "Starting training loop...\n",
      "\n",
      "Starting batch 0...\n",
      "\n",
      "WEIGHTED IMAGES min: 0 | max: 2434 | mean: 61\n",
      "INT IMAGES min: 0 | max: 2434 | mean: 61\n",
      "WEIGHTED LABELS min: 0 | max: 65 | mean: 0\n",
      "LABELS min: 0 | max: 65 | mean: 0\n",
      "best pair:  9\n",
      "means list [0.0005086153207390492, 0.0006779831257533146, 0.0003220197076061055, 0.0, 0.0006360810241800378, 0.0, 0.0003517220854414112, 0.0004295796256520405, 0.0007530415820148569, 0.000874969151728625, 0.00015966298827705213, 0.000637732506643047]\n",
      "Batch 0  |  Loss = 19.0270  |  Positive Accuracy = 0.0004  |  Total Accuracy = 0.7587\n",
      "\n",
      "Number of Graph Ops Stored:  1471\n",
      "Starting batch 1...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTED IMAGES min: 0 | max: 2087 | mean: 17\n",
      "INT IMAGES min: 0 | max: 2087 | mean: 17\n",
      "WEIGHTED LABELS min: 0 | max: 65 | mean: 0\n",
      "LABELS min: 0 | max: 65 | mean: 0\n",
      "best pair:  5\n",
      "means list [0.0005991919468602342, 0.00011010155557769238, 8.443182657702821e-05, 2.0596898107145064e-05, 0.0006362479549172878, nan, 0.0011031439602868175, 0.0003717882123568882, 0.0, 0.0, 0.0003914164671210168, 0.00042681908438027723]\n",
      "Batch 1  |  Loss = 15.5133  |  Positive Accuracy = nan  |  Total Accuracy = 0.9157\n",
      "\n",
      "Number of Graph Ops Stored:  1471\n",
      "Starting batch 2...\n",
      "\n",
      "WEIGHTED IMAGES min: 0 | max: 1959 | mean: 5\n",
      "INT IMAGES min: 0 | max: 1959 | mean: 5\n",
      "WEIGHTED LABELS min: 0 | max: 65 | mean: 0\n",
      "LABELS min: 0 | max: 65 | mean: 0\n",
      "best pair:  7\n",
      "means list [0.00016831373680540528, 0.00022140936284843144, 0.00032107485983847464, 0.00013757365923004609, 0.0001248350394122053, 9.433072351664937e-05, 0.0, 0.0003569728700618753, 0.00019890635994462814, 0.00010994228030284101, 0.0001878956900556206, 0.0001443183663160933]\n",
      "Batch 2  |  Loss = 15.3846  |  Positive Accuracy = 0.0002  |  Total Accuracy = 0.9515\n",
      "\n",
      "Number of Graph Ops Stored:  1471\n",
      "Starting batch 3...\n",
      "\n",
      "WEIGHTED IMAGES min: 0 | max: 1797 | mean: 3\n",
      "INT IMAGES min: 0 | max: 1797 | mean: 3\n",
      "WEIGHTED LABELS min: 0 | max: 65 | mean: 0\n",
      "LABELS min: 0 | max: 65 | mean: 0\n",
      "best pair:  9\n",
      "means list [3.962346388321267e-05, 3.7334328915437745e-05, 0.0, 0.00019610727067706035, 0.00011790135586559245, 3.825330891122082e-05, 0.00013479744092242927, 5.393572721169029e-05, 4.099998975000256e-05, 0.0003957261574990107, 0.0, 5.760593452620977e-05]\n",
      "Batch 3  |  Loss = 18.8196  |  Positive Accuracy = 0.0001  |  Total Accuracy = 0.9629\n",
      "\n",
      "Number of Graph Ops Stored:  1471\n",
      "Starting batch 4...\n",
      "\n",
      "WEIGHTED IMAGES min: 0 | max: 1391 | mean: 2\n",
      "INT IMAGES min: 0 | max: 1391 | mean: 2\n",
      "WEIGHTED LABELS min: 0 | max: 65 | mean: 0\n",
      "LABELS min: 0 | max: 65 | mean: 0\n",
      "best pair:  11\n",
      "means list [3.733393067622415e-05, 7.44195276840643e-05, 0.0, 0.0, 9.878567968796472e-05, 0.0001978108927864961, 0.0, 0.00016972689399783985, 1.1654196676223107e-05, 0.0, 1.461550266367536e-05, 0.0002455253013823074]\n",
      "Batch 4  |  Loss = 16.8118  |  Positive Accuracy = 0.0001  |  Total Accuracy = 0.9659\n",
      "\n",
      "Number of Graph Ops Stored:  1471\n",
      "Starting batch 5...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_net(hp, data_set, create_model, create_optimizer):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
    "    accuracy on the CIFAR-10 validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - create_model: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = create_model()\n",
    "    - create_optimizer: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = create_optimizer()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints progress during training\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "        \n",
    "    with tf.device(hp.device):\n",
    "        # Construct the computational graph we will use to train the model. We\n",
    "        # use the create_model to construct the model, declare placeholders for\n",
    "        # the data and labels\n",
    "        x = tf.placeholder(hp.data_type, [None, 2710, 3384, 3])\n",
    "#         y = tf.placeholder(np.dtype('int32'), [None, 2710, 3384])\n",
    "        y = tf.placeholder(hp.data_type, [None, 2710, 3384])\n",
    "\n",
    "        # We need a place holder to explicitly specify if the model is in the training\n",
    "        # phase or not. This is because a number of layers behaves differently in\n",
    "        # training and in testing, e.g., dropout and batch normalization.\n",
    "        # We pass this variable to the computation graph through feed_dict as shown below.\n",
    "        is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "        # Use the model function to build the forward pass.\n",
    "        output = tf.squeeze(create_model(x, is_training))\n",
    "\n",
    "        # Full loss type is better\n",
    "        if hp.loss_type == \"full\":\n",
    "#             loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=y, dim=0))\n",
    "            loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(logits=output, targets=y, pos_weight=40))\n",
    "#             loss = 1/tf.reduce_mean(tf.cast(tf.equal(tf.cast(tf.squeeze(output), tf.int32), tf.squeeze(y)), tf.int32))\n",
    "        if hp.loss_type == \"fast\":\n",
    "            loss = tf.reduce_mean(tf.square(y - output))\n",
    "#             loss = tf.reduce_mean(y - output)\n",
    "\n",
    "        # Use the create_optimizer to construct an Optimizer, then use the optimizer\n",
    "        # to set up the training step. Asking TensorFlow to evaluate the\n",
    "        # train_op returned by optimizer.minimize(loss) will cause us to make a\n",
    "        # single update step using the current minibatch of data.\n",
    "\n",
    "        # Note that we use tf.control_dependencies to force the model to run\n",
    "        # the tf.GraphKeys.UPDATE_OPS at each training step. tf.GraphKeys.UPDATE_OPS\n",
    "        # holds the operators that update the states of the network.\n",
    "        # For example, the tf.layers.batch_normalization function adds the running mean\n",
    "        # and variance update operators to tf.GraphKeys.UPDATE_OPS.\n",
    "\n",
    "        optimizer = create_optimizer(hp)\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer_step = optimizer.minimize(loss)\n",
    "\n",
    "                \n",
    "                \n",
    "    # Now we can run the computational graph many times to train the model.\n",
    "    # When we call sess.run we ask it to evaluate train_op, which causes the model to update.\n",
    "    # # Total data to train on = num_epochs*batch_size\n",
    "    \n",
    "#     data_iterator = data_set.__iter__()\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    train_output = None    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        if hp.use_saved_model:\n",
    "            load_saved_model(sess)   \n",
    "        if hp.save_model:\n",
    "             builder = create_saver(sess) \n",
    "                  \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"\\nStarting training loop...\\n\")\n",
    "        \n",
    "        for epoch in range(hp.num_epochs):\n",
    "            print('Starting batch %d...\\n' % epoch)\n",
    "            data_set.update_iterator()\n",
    "            train_images, train_labels = data_set.get_images()\n",
    "            feed_dict = {x: train_images, y: train_labels, is_training: 1}\n",
    "            loss_value, _, train_output = sess.run([loss, optimizer_step, output], feed_dict=feed_dict)\n",
    "            accuracy, total_acc = check_accuracy(hp, sess, train_output, train_labels)\n",
    "            print('Batch %d  |  Loss = %.4f  |  Positive Accuracy = %.4f  |  Total Accuracy = %.4f\\n' % (epoch, loss_value, accuracy, total_acc))\n",
    "            gc.collect()\n",
    "#             print_graph_size()\n",
    "                \n",
    "        data_set.reset_data_set()\n",
    "        save_model(sess, builder)\n",
    "    \n",
    "\n",
    "data_set = CVPR(hp.root, transform = tf.convert_to_tensor)              \n",
    "train_net(hp, data_set, create_model, create_optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
