{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net for Semantic Image Segmentation\n",
    "#### Kaggle Competition (https://www.kaggle.com/c/cvpr-2018-autonomous-driving)\n",
    "#### Stanford course CS231n\n",
    "#### Aristos Athens (aristos@stanford.edu)\n",
    "#### Alice Li (ali2@stanford.edu)\n",
    "#### Amelia Bian (qb1@stanford.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import gc\n",
    "from PIL import Image\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.python.keras.applications import vgg16, inception_v3, resnet50, mobilenet\n",
    "from tensorflow.python.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.python.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "%matplotlib inline\n",
    "# CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters:\n",
    "    \"\"\"\n",
    "    Object to hold all hyperparameters. Makes passing parameters between functions easier.\n",
    "    Many of these might not be strictly necessary.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # General params\n",
    "        self.data_type = tf.float32\n",
    "        self.root = '../data/cvpr-2018-autonomous-driving/train_color'\n",
    "        self.device = '/cpu:0'\n",
    "        \n",
    "        # Training params\n",
    "        self.optimizer = \"Adam\" # options: SGD, RMSProp, Momentum, Adam, Adagrad\n",
    "        self.learning_rate = 1e-5\n",
    "        self.lr_decay = 0.9\n",
    "        self.loss_type = \"full\"  # options: \"fast\", \"full\"\n",
    "        self.momentum = 0.8\n",
    "        self.use_Nesterov = True\n",
    "        self.init_scale = 2.0\n",
    "        self.num_epochs = 30  # Total data to train on = num_epochs*batch_size\n",
    "        \n",
    "        # Data loader params\n",
    "        self.shuffle_data = True  # Currently doesn't do anything\n",
    "        self.preload = False\n",
    "        self.batch_size = 15\n",
    "        self.num_files_to_load = self.num_epochs * self.batch_size\n",
    "        \n",
    "        self.num_classes = 20  # This value is probably wrong\n",
    "  \n",
    "        \n",
    "              \n",
    "hp = HyperParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we create a customized data loader for the CVPR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVPR(object):\n",
    "    \"\"\"\n",
    "    A customized data loader for CVPR.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform=None, batch_size = 10):\n",
    "        \"\"\" Intialize the CVPR dataset\n",
    "        \n",
    "        Args:\n",
    "            - root: root directory of the dataset\n",
    "            - tranform: a custom tranform function\n",
    "            - preload: if preload the dataset into memory\n",
    "        \"\"\"\n",
    "        # Important - This tracks which batch we are on. Used for loading as we go (as opposed to preloading)\n",
    "        self.batch_num = 0\n",
    "        \n",
    "        # read filenames\n",
    "        self.filenames = []\n",
    "        filenames = glob.glob(osp.join(hp.root, '*.jpg'))\n",
    "        random.shuffle(filenames)\n",
    "        for fn in filenames:\n",
    "            lbl = fn[:-4] + '_instanceIds.png'\n",
    "            lbl = lbl.replace('color', 'label')\n",
    "            self.filenames.append((fn, lbl)) # (filename, label) pair\n",
    "            if len(self.filenames) >= hp.num_files_to_load: \n",
    "                break\n",
    "                \n",
    "        self.labels = []\n",
    "        self.images = []\n",
    "        if hp.preload:\n",
    "            self._preload()\n",
    "            \n",
    "        self.len = len(self.filenames)\n",
    "                              \n",
    "    def _preload(self):\n",
    "        \"\"\"\n",
    "        Preload dataset to memory\n",
    "        \"\"\"\n",
    "        for image_fn, label in self.filenames:\n",
    "            # load images\n",
    "            image = Image.open(image_fn)\n",
    "            self.images.append(np.asarray(image))            # avoid too many opened files bug\n",
    "            image.close()\n",
    "            # load labels\n",
    "            image = Image.open(label)\n",
    "            self.labels.append((np.asarray(image)/1000).astype(int))             # avoid too many opened files bug\n",
    "            image.close()\n",
    "    \n",
    "        assert len(self.images) == len(self.labels), 'Got different numbers of data and labels'\n",
    "#         assert self.images[0].shape == self.labels[0].shape, 'Got data and labels with different shapes'\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset\n",
    "        \"\"\"\n",
    "        if self.images is not None:\n",
    "            # If dataset is preloaded\n",
    "            image = self.images[index]\n",
    "            label = self.labels[index]\n",
    "        else:\n",
    "            # If on-demand data loading\n",
    "            image_fn, label = self.filenames[index]\n",
    "            image = Image.open(image_fn)\n",
    "            label = Image.open(label)\n",
    "            \n",
    "        # May use transform function to transform samples\n",
    "        # e.g., random crop, whitening\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "        # return image and label\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        N, B = hp.num_files_to_load, hp.batch_size\n",
    "        return iter((self.images[i:i+B], self.labels[i:i+B]) for i in range(0, N, B))\n",
    "    \n",
    "    def update_iterator(self):\n",
    "        # If a batch has already been loaded free the last batch\n",
    "        if len(self.images) != 0:\n",
    "            for image in self.images:\n",
    "                del image\n",
    "            for label in self.labels:\n",
    "                del label\n",
    "        del self.images\n",
    "        del self.labels\n",
    "        self.images = []\n",
    "        self.labels =[]\n",
    "        gc.collect()\n",
    "        i = self.batch_num*hp.batch_size\n",
    "        train_filenames = self.filenames[i:i + hp.batch_size]\n",
    "        for image_fn, label in train_filenames:\n",
    "            image = Image.open(image_fn)\n",
    "            self.images.append(self.normalize_image(np.asarray(image)))\n",
    "            image.close()\n",
    "            del image\n",
    "            image = Image.open(label)\n",
    "            self.labels.append(np.asarray(image).astype(int))\n",
    "            image.close()\n",
    "            del image\n",
    "            gc.collect()\n",
    "        self.batch_num += 1\n",
    "        \n",
    "    def get_images(self):\n",
    "        return self.images, self.labels\n",
    "\n",
    "    def normalize_image(self, image):\n",
    "        # Normalize the data: subtract the mean pixel and divide by std\n",
    "        mean_pixel = image.mean(axis=(0, 1, 2), keepdims=True)\n",
    "        std_pixel = image.std(axis=(0, 1, 2), keepdims=True)\n",
    "        image = (image - mean_pixel) / std_pixel\n",
    "        return image\n",
    "    \n",
    "    def reset_data_set(self):\n",
    "        self.batch_num = 0\n",
    "        del self.images\n",
    "        del self.labels\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a data set object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hp.preload:\n",
    "    data_set = CVPR(hp.root, transform = tf.convert_to_tensor)\n",
    "    print(len(data_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show_image() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to show images\n",
    "# Takes an image as a tensor\n",
    "\n",
    "def show_image(hp, image_tensor):\n",
    "    with tf.Session() as session:\n",
    "        image_model = tf.global_variables_initializer()\n",
    "        session.run(hp.image_model)\n",
    "        result = session.run(image_tensor)\n",
    "    plt.imshow(result)\n",
    "    plt.show()\n",
    "\n",
    "# Print a random example image\n",
    "if hp.preload:\n",
    "    index = random.randrange(0, len(data_set))\n",
    "    image, label = data_set.__getitem__(index)\n",
    "    show_image(hp, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory():\n",
    "    CPU_Pct=str(round(float(os.popen('''grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {print usage }' ''').readline()),2))\n",
    "\n",
    "    #print results\n",
    "    print(\"CPU Usage = \" + CPU_Pct)\n",
    "    mem=str(os.popen('free -t -m').readlines())\n",
    "    \"\"\"\n",
    "    Get a whole line of memory output, it will be something like below\n",
    "    ['             total       used       free     shared    buffers     cached\\n', \n",
    "    'Mem:           925        591        334         14         30        355\\n', \n",
    "    '-/+ buffers/cache:        205        719\\n', \n",
    "    'Swap:           99          0         99\\n', \n",
    "    'Total:        1025        591        434\\n']\n",
    "     So, we need total memory, usage and free memory.\n",
    "     We should find the index of capital T which is unique at this string\n",
    "    \"\"\"\n",
    "    T_ind=mem.index('T')\n",
    "    \"\"\"\n",
    "    Than, we can recreate the string with this information. After T we have,\n",
    "    \"Total:        \" which has 14 characters, so we can start from index of T +14\n",
    "    and last 4 characters are also not necessary.\n",
    "    We can create a new sub-string using this information\n",
    "    \"\"\"\n",
    "    mem_G=mem[T_ind+14:-4]\n",
    "    \"\"\"\n",
    "    The result will be like\n",
    "    1025        603        422\n",
    "    we need to find first index of the first space, and we can start our substring\n",
    "    from from 0 to this index number, this will give us the string of total memory\n",
    "    \"\"\"\n",
    "    S1_ind=mem_G.index(' ')\n",
    "    mem_T=mem_G[0:S1_ind]\n",
    "    \"\"\"\n",
    "    Similarly we will create a new sub-string, which will start at the second value. \n",
    "    The resulting string will be like\n",
    "    603        422\n",
    "    Again, we should find the index of first space and than the \n",
    "    take the Used Memory and Free memory.\n",
    "    \"\"\"\n",
    "    mem_G1=mem_G[S1_ind+8:]\n",
    "    S2_ind=mem_G1.index(' ')\n",
    "    mem_U=mem_G1[0:S2_ind]\n",
    "\n",
    "    mem_F=mem_G1[S2_ind+8:]\n",
    "    print ('Summary = ', mem_G)\n",
    "    print ('Total Memory = ', mem_T, ' MB')\n",
    "    print ('Used Memory = ', mem_U, ' MB')\n",
    "    print ('Free Memory = ', mem_F, ' MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(inputs, is_training):\n",
    "    \n",
    "    initializer = tf.variance_scaling_initializer(scale=hp.init_scale)\n",
    "    input_shape = (2710, 3384, 3)\n",
    "    \n",
    "    # Transfer learning example here:\n",
    "    # https://deeplearningsandbox.com/how-to-use-transfer-learning-and-fine-tuning-in-keras-and-tensorflow-to-build-an-image-recognition-94b0b02444f2\n",
    "#     base = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "#     base = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "#     base = resnet50.ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    \n",
    "    # Not 100% sure what this line does\n",
    "#     model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     from pympler import asizeof\n",
    "#     print(asizeof.asizeof(base))\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "#     for layer in base.layers:\n",
    "#         layer.trainable = False\n",
    "#     model.add(base)\n",
    "    \n",
    "    # Add our own layers to the layers list within the model\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(0.01)\n",
    "    layers = [\n",
    "        # Downsampling layers\n",
    "        tf.keras.layers.Conv2D(10, (3, 3), kernel_initializer = initializer, input_shape=input_shape, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "#         tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.AveragePooling2D(pool_size=(4, 4), strides = (4, 4), padding = \"valid\"),\n",
    "        tf.keras.layers.Conv2D(10, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.Conv2D(12, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "#         tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides = (2, 2), padding = \"valid\"),\n",
    "        tf.keras.layers.Conv2D(15, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.Conv2D(15, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "#         tf.keras.layers.BatchNormalization(),\n",
    "#         tf.keras.layers.Dropout(rate=0.5),\n",
    "        # Upsampling layers\n",
    "        tf.keras.layers.UpSampling2D(size=(2, 2)),\n",
    "        tf.keras.layers.ZeroPadding2D(padding=((0,1),(0,0))),\n",
    "        tf.keras.layers.Conv2D(12, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.Conv2D(10, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.UpSampling2D(size=(4, 4)),\n",
    "        tf.keras.layers.ZeroPadding2D(padding=((0,2),(0,0))),\n",
    "        tf.keras.layers.Conv2D(10, (3, 3), kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "        tf.keras.layers.Conv2D(1, (3, 3), activation=None, kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "            \n",
    "        \n",
    "#         Upsampling layers\n",
    "#         tf.keras.layers.UpSampling2D(size=(2, 2)),\n",
    "#         tf.keras.layers.Conv2D(12, (3, 3), activation=\"relu\", kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "#         tf.keras.layers.UpSampling2D(size=(2, 2)),\n",
    "#         tf.keras.layers.ZeroPadding2D(padding=((1,1),(1,1))),\n",
    "#         tf.keras.layers.Conv2D(12, (3, 3), activation=\"relu\", kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "#         tf.keras.layers.UpSampling2D(size=(2, 2)),\n",
    "#         tf.keras.layers.ZeroPadding2D(padding=((0,1),(1,1))),\n",
    "#         tf.keras.layers.Conv2D(10, (3, 3), activation=\"relu\", kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "# #         tf.keras.layers.ZeroPadding2D(padding=((0,0),(1,1))),\n",
    "#         tf.keras.layers.UpSampling2D(size=(2, 2)),\n",
    "#         tf.keras.layers.Conv2D(10, (3, 3), activation=\"relu\", kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "#         tf.keras.layers.UpSampling2D(size=(2, 2)),\n",
    "#         tf.keras.layers.ZeroPadding2D(padding=((1,1),(0,0))),\n",
    "#         tf.keras.layers.Conv2D(1, (3, 3), activation=None, kernel_initializer = initializer, padding = \"SAME\", kernel_regularizer=regularizer),\n",
    "    \n",
    "    ]\n",
    "#     model.add(layers)\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "\n",
    "    return model(inputs)\n",
    "\n",
    "def create_optimizer(hp):\n",
    "    \n",
    "    optimizer = None\n",
    "    if hp.optimizer == \"Momentum\":    \n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate=hp.learning_rate, momentum=hp.momentum, use_nesterov=hp.use_Nesterov)\n",
    "    if hp.optimizer == \"Adam\":    \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=hp.learning_rate, beta1=hp.lr_decay)\n",
    "    if hp.optimizer == \"AdaGrad\":\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate=hp.learning_rate)\n",
    "    if hp.optimizer == \"SGD\":\n",
    "        optimizer = tf.train.SGDOptimizer(learning_rate=hp.learning_rate, lr_decay=hp.lr_decay)\n",
    "    if hp.optimizer == \"RMSProp\":\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=hp.learning_rate, decay=hp.lr_decay, momentum=hp.momentum, epsilon=1e-10)\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Starting training loop...\n",
      "\n",
      "CPU Usage = 22.72\n",
      "Summary =  00710         574       98130\n",
      "Total Memory =  00710  MB\n",
      "Used Memory =    MB\n",
      "Free Memory =     98130  MB\n",
      "Starting batch 0...\n",
      "()\n",
      "Batch 0 Loss = 17760610.0000 \n",
      "\n",
      "CPU Usage = 22.78\n",
      "Summary =  00710        4991       93675\n",
      "Total Memory =  00710  MB\n",
      "Used Memory =  4991  MB\n",
      "Free Memory =  3675  MB\n",
      "Starting batch 1...\n",
      "()\n",
      "Batch 1 Loss = 15380878.0000 \n",
      "\n",
      "CPU Usage = 22.84\n",
      "Summary =  00710        5156       93470\n",
      "Total Memory =  00710  MB\n",
      "Used Memory =  5156  MB\n",
      "Free Memory =  3470  MB\n",
      "Starting batch 2...\n",
      "()\n",
      "Batch 2 Loss = 21724118.0000 \n",
      "\n",
      "CPU Usage = 22.89\n",
      "Summary =  00710        5246       93346\n",
      "Total Memory =  00710  MB\n",
      "Used Memory =  5246  MB\n",
      "Free Memory =  3346  MB\n",
      "Starting batch 3...\n",
      "()\n",
      "Batch 3 Loss = 30240658.0000 \n",
      "\n",
      "CPU Usage = 22.95\n",
      "Summary =  00710        5326       93237\n",
      "Total Memory =  00710  MB\n",
      "Used Memory =  5326  MB\n",
      "Free Memory =  3237  MB\n",
      "Starting batch 4...\n",
      "()\n",
      "Batch 4 Loss = 31719504.0000 \n",
      "\n",
      "CPU Usage = 23.0\n",
      "Summary =  00710        5415       93114\n",
      "Total Memory =  00710  MB\n",
      "Used Memory =  5415  MB\n",
      "Free Memory =  3114  MB\n",
      "Starting batch 5...\n",
      "()\n",
      "Batch 5 Loss = 18246660.0000 \n",
      "\n",
      "CPU Usage = 23.05\n",
      "Summary =  00710        5500       92996\n",
      "Total Memory =  00710  MB\n",
      "Used Memory =  5500  MB\n",
      "Free Memory =  2996  MB\n",
      "Starting batch 6...\n",
      "()\n",
      "Batch 6 Loss = 25180372.0000 \n",
      "\n",
      "CPU Usage = 23.1\n",
      "Summary =  00710        5569       92896\n",
      "Total Memory =  00710  MB\n",
      "Used Memory =  5569  MB\n",
      "Free Memory =  2896  MB\n",
      "Starting batch 7...\n"
     ]
    }
   ],
   "source": [
    "def train_net(hp, data_set, create_model, create_optimizer):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
    "    accuracy on the CIFAR-10 validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - create_model: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = create_model()\n",
    "    - create_optimizer: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = create_optimizer()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints progress during training\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()    \n",
    "    with tf.device(hp.device):\n",
    "        # Construct the computational graph we will use to train the model. We\n",
    "        # use the create_model to construct the model, declare placeholders for\n",
    "        # the data and labels\n",
    "        x = tf.placeholder(hp.data_type, [None, 2710, 3384, 3])\n",
    "        y = tf.placeholder(np.dtype('int32'), [None, 2710, 3384])\n",
    "        \n",
    "        # We need a place holder to explicitly specify if the model is in the training\n",
    "        # phase or not. This is because a number of layers behaves differently in\n",
    "        # training and in testing, e.g., dropout and batch normalization.\n",
    "        # We pass this variable to the computation graph through feed_dict as shown below.\n",
    "        is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "        \n",
    "        # Use the model function to build the forward pass.\n",
    "        output = tf.squeeze(create_model(x, is_training))\n",
    "        \n",
    "        # Full loss type is better\n",
    "        if hp.loss_type == \"full\":\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=y)) #, dim=0))\n",
    "        if hp.loss_type == \"fast\":\n",
    "            loss = tf.reduce_mean(tf.square(y - output))\n",
    "#             loss = tf.reduce_mean(y - output)\n",
    "\n",
    "        # Use the create_optimizer to construct an Optimizer, then use the optimizer\n",
    "        # to set up the training step. Asking TensorFlow to evaluate the\n",
    "        # train_op returned by optimizer.minimize(loss) will cause us to make a\n",
    "        # single update step using the current minibatch of data.\n",
    "        \n",
    "        # Note that we use tf.control_dependencies to force the model to run\n",
    "        # the tf.GraphKeys.UPDATE_OPS at each training step. tf.GraphKeys.UPDATE_OPS\n",
    "        # holds the operators that update the states of the network.\n",
    "        # For example, the tf.layers.batch_normalization function adds the running mean\n",
    "        # and variance update operators to tf.GraphKeys.UPDATE_OPS.\n",
    "        \n",
    "        optimizer = create_optimizer(hp)\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer_step = optimizer.minimize(loss)\n",
    "\n",
    "    # Now we can run the computational graph many times to train the model.\n",
    "    # When we call sess.run we ask it to evaluate train_op, which causes the model to update.\n",
    "    # # Total data to train on = num_epochs*batch_size\n",
    "    \n",
    "    import psutil\n",
    "    \n",
    "#     data_iterator = data_set.__iter__()\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(\"Starting training loop...\\n\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(hp.num_epochs):\n",
    "            print_memory()\n",
    "#             print('Memory stats:', psutil.virtual_memory())\n",
    "            print('Starting batch %d...' % epoch)\n",
    "            data_set.update_iterator()\n",
    "#             train_images, train_labels = next(data_iterator)\n",
    "            train_images, train_labels = data_set.get_images()\n",
    "            feed_dict = {x: train_images, y: train_labels, is_training: 1}\n",
    "            loss_value, _ = sess.run([loss, optimizer_step], feed_dict=feed_dict)\n",
    "            print(loss_value.shape)\n",
    "            print('Batch %d Loss = %.4f \\n' % (epoch, loss_value))\n",
    "            \n",
    "            \n",
    "    data_set.reset_data_set()\n",
    "    \n",
    "\n",
    "data_set = CVPR(hp.root, transform = tf.convert_to_tensor)              \n",
    "train_net(hp, data_set, create_model, create_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
